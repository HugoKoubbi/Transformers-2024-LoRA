{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2QbuxvSYU1y"
      },
      "source": [
        "# Exploring the capabilities of the Transformers model.\n",
        "\n",
        "This notebook implements a Transformers architecture with only layer which is repeated a certain number of time; we investigate how this architecture succeeds to learn modular addition. Besides, we investigate the learned parameters, the impact of depth of this Transformers. We remind that the Transformers model we propose is the following\n",
        "$$x_{k}^{i+1}=x_{k}^{i}+\\sum_{j=1}^{n}\\frac{e^{\\langle Qx_{k}^{i},K x_{j}^{i}\\rangle}}{\\sum_{t=1}^{n}e^{\\langle Qx_{t}^{i},K x_{k}^{i}\\rangle}}Vx_{j}^{i}.$$\n",
        "We can see this model as an equivalent of the ResNet neural networks for Transformers with a same attention head which is repeated through the layers. We investigate the different performances according to depth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OOT42OluMcC"
      },
      "source": [
        "# Imports and implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAUdpAMIC1-w"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_q1N25u_FdDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c56f65-9939-4d8e-a955-4ffb00983864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 25 08:31:06 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P0              27W /  70W |    655MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLAWL79fTbH4",
        "outputId": "eabc31bc-f962-4612-9dd0-8ac59bdb3d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eErNt-8AGUaO",
        "outputId": "247f35b0-d666-4f69-e136-f650734b2691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HzL5FOpyGUP8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import einops\n",
        "from tqdm.auto import tqdm\n",
        "import math\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z=sns.color_palette(\"Paired\")"
      ],
      "metadata": {
        "id": "sAg79ck6m1-f"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2HgOcrFGd4E"
      },
      "source": [
        "### Transformer implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "anHimo51L3Cc"
      },
      "outputs": [],
      "source": [
        "alpha=1\n",
        "# Embed & Unembed\n",
        "class Embed(nn.Module):\n",
        "    def __init__(self, d_vocab, d_model):\n",
        "        super().__init__()\n",
        "        self.W_E = nn.Parameter(alpha*torch.randn(d_model, d_vocab)/np.sqrt(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.einsum('dbp -> bpd', self.W_E[:, x])\n",
        "\n",
        "class Unembed(nn.Module):\n",
        "    def __init__(self, d_vocab, d_model):\n",
        "        super().__init__()\n",
        "        self.W_U = nn.Parameter(alpha*torch.randn(d_model, d_vocab)/np.sqrt(d_vocab))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (x @ self.W_U)\n",
        "\n",
        "# Positional Embeddings\n",
        "class PosEmbed(nn.Module):\n",
        "    def __init__(self, max_ctx, d_model):\n",
        "        super().__init__()\n",
        "        self.W_pos = nn.Parameter(alpha*torch.randn(max_ctx, d_model)/np.sqrt(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x+self.W_pos[:x.shape[-2]]\n",
        "\n",
        "# LayerNorm\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, epsilon = 1e-4, model=[None]):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.w_ln = nn.Parameter(alpha*torch.ones(d_model))\n",
        "        self.b_ln = nn.Parameter(alpha*torch.zeros(d_model))\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.model[0].use_ln:\n",
        "            x = x - x.mean(axis=-1)[..., None]\n",
        "            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n",
        "            x = x * self.w_ln\n",
        "            x = x + self.b_ln\n",
        "            return x\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "# Attention\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_head, n_ctx, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.W_K = nn.Linear(d_head, d_model)\n",
        "        self.W_Q = nn.Linear(d_head, d_model)\n",
        "        self.W_V = nn.Linear(d_head, d_model)\n",
        "        self.W_O = nn.Linear(d_model, d_head )\n",
        "        self.register_buffer('mask', torch.tril(torch.ones((n_ctx, n_ctx))))\n",
        "        self.d_head = d_head\n",
        "\n",
        "    def forward(self, x):\n",
        "      ###computation of the scalar products\n",
        "        W_k=self.W_K.weight\n",
        "        W_q=self.W_Q.weight\n",
        "        W_v=self.W_V.weight\n",
        "        W_o=self.W_O.weight\n",
        "        k =torch.einsum('hd,bpd->bph', W_k, x)\n",
        "        q = torch.einsum('hd,bpd->bph',W_q, x)\n",
        "        v = torch.einsum('hd,bpd->bph', W_v, x)\n",
        "        attn_scores_pre = torch.einsum('bph,bqh->bqp', k, q)\n",
        "        attn_scores_masked = torch.tril(attn_scores_pre) - 1e10 * (1 - self.mask[:x.shape[-2], :x.shape[-2]])\n",
        "        attn_matrix = F.softmax((attn_scores_masked/np.sqrt(self.d_head)), dim=-1)\n",
        "        z = torch.einsum('bph,bqp->bqh', v, attn_matrix)\n",
        "        z_flat = einops.rearrange(z, 'b q h -> b q (h)')\n",
        "        out = torch.einsum('df,bqf->bqd', W_o, z_flat)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_head, n_ctx, model):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_dim: dimension of embeding vector output\n",
        "            n_heads: number of self attention heads\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.d_model = d_model    #512 dim\n",
        "        self.num_heads = num_heads   #8\n",
        "        self.single_head_dim = int(self.d_model / self.num_heads)   #512/8 = 64  . each key,query, value will be of 64d\n",
        "\n",
        "        #key,query and value matrixes    #64 x 64\n",
        "        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False)  # single key matrix for all 8 keys #512x512\n",
        "        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False)\n",
        "        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False)\n",
        "        self.out = nn.Linear(self.num_heads*self.single_head_dim ,self.d_model)\n",
        "\n",
        "    def forward(self,x,mask=None):    #batch_size x sequence_length x embedding_dim    # 32 x 10 x 512\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           key : key vector\n",
        "           query : query vector\n",
        "           value : value vector\n",
        "           mask: mask for decoder\n",
        "\n",
        "        Returns:\n",
        "           output vector from multihead attention\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        seq_length = x.size(1)\n",
        "\n",
        "        # query dimension can change in decoder during inference.\n",
        "        # so we cant take general seq_length\n",
        "        #seq_length_query = query.size(1)\n",
        "\n",
        "        # 32x10x512\n",
        "        key = x.view(batch_size, seq_length, self.num_heads, self.single_head_dim)  #batch_size x sequence_length x n_heads x single_head_dim = (32x10x8x64)\n",
        "        query = x.view(batch_size, seq_length, self.num_heads, self.single_head_dim) #(32x10x8x64)\n",
        "        value = x.view(batch_size, seq_length, self.num_heads, self.single_head_dim) #(32x10x8x64)\n",
        "\n",
        "        k = self.key_matrix(key)       # (32x10x8x64)\n",
        "        q = self.query_matrix(query)\n",
        "        v = self.value_matrix(value)\n",
        "\n",
        "        q = q.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)    # (32 x 8 x 10 x 64)\n",
        "        k = k.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
        "        v = v.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
        "\n",
        "        # computes attention\n",
        "        # adjust key for matrix multiplication\n",
        "        k_adjusted = k.transpose(-1,-2)  #(batch_size, n_heads, single_head_dim, seq_ken)  #(32 x 8 x 64 x 10)\n",
        "        product = torch.matmul(q, k_adjusted)  #(32 x 8 x 10 x 64) x (32 x 8 x 64 x 10) = #(32x8x10x10)\n",
        "        # fill those positions of product matrix as (-1e20) where mask positions are 0\n",
        "        if mask is not None:\n",
        "             product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "        #divising by square root of key dimension\n",
        "        product = product / math.sqrt(self.single_head_dim)\n",
        "        #applying softmax\n",
        "        scores = F.softmax(product, dim=-1)\n",
        "        #mutiply with value matrix\n",
        "        scores = torch.matmul(scores, v)\n",
        "        #concatenated output\n",
        "        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length, self.single_head_dim*self.num_heads)  # (32x8x10x64) -> (32x10x8x64)  -> (32,10,512)\n",
        "\n",
        "        output = self.out(concat) #(32,10,512) -> (32,10,512)\n",
        "\n",
        "        return output\n",
        "# MLP Layers\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, d_model, d_mlp, act_type, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.W_in = nn.Parameter(alpha*torch.randn(d_mlp, d_model)/np.sqrt(d_model))\n",
        "        self.b_in = nn.Parameter(alpha*torch.zeros(d_mlp))\n",
        "        self.W_out = nn.Parameter(alpha*torch.randn(d_model, d_mlp)/np.sqrt(d_model))\n",
        "        self.b_out = nn.Parameter(torch.zeros(d_model))\n",
        "        self.act_type = act_type\n",
        "        self.ln = LayerNorm(d_mlp, model=self.model)\n",
        "        assert act_type in ['ReLU', 'GeLU']\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.einsum('md,bpd->bpm', self.W_in, x) + self.b_in\n",
        "        if self.act_type=='ReLU':\n",
        "            x = F.relu(x)\n",
        "        elif self.act_type=='GeLU':\n",
        "            x = F.gelu(x)\n",
        "        x = torch.einsum('dm,bpm->bpd', self.W_out, x) + self.b_out\n",
        "        return x\n",
        "\n",
        "# Transformer Block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        # self.ln1 = LayerNorm(d_model, model=self.model)\n",
        "        self.attn = MultiHeadAttention(d_model, num_heads, d_head, n_ctx, model=self.model)\n",
        "        # self.ln2 = LayerNorm(d_model, model=self.model)\n",
        "        self.mlp = MLP(d_model, d_mlp, act_type, model=self.model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(x)\n",
        "        #x = x + self.mlp(x)\n",
        "        return x\n",
        "\n",
        "# Full transformer\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers, d_vocab, d_model, d_mlp, d_head, num_heads, n_ctx, act_type,depth, use_cache=False, use_ln=True):\n",
        "        super().__init__()\n",
        "        self.cache = {}\n",
        "        self.use_cache = use_cache\n",
        "        self.depth=depth\n",
        "\n",
        "        self.embed = Embed(d_vocab, d_model)\n",
        "        self.pos_embed = PosEmbed(n_ctx, d_model)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model=[self]) for i in range(num_layers)])\n",
        "        self.ln = LayerNorm(d_model, model=[self])\n",
        "        self.unembed = Unembed(d_vocab, d_model)\n",
        "        self.use_ln = use_ln\n",
        "\n",
        "        #for name, module in self.named_modules():\n",
        "            #if type(module)==HookPoint:\n",
        "             #   module.give_name(name)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = self.pos_embed(x)\n",
        "        for block in self.blocks:\n",
        "          for i in range(self.depth):\n",
        "            x = block(x)\n",
        "        x = self.ln(x)\n",
        "        x = self.unembed(x)\n",
        "        ##mettre un pooling\n",
        "\n",
        "        return x\n",
        "\n",
        "    def set_use_cache(self, use_cache):\n",
        "        self.use_cache = use_cache\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G90QwHKiGddr"
      },
      "source": [
        "### Baseline training run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Z7mKIz7daP0y"
      },
      "outputs": [],
      "source": [
        "def full_loss(model, data, device):\n",
        "    loader = torch.utils.data.DataLoader(data, batch_size=len(data), shuffle=False)\n",
        "    # Take the final position only\n",
        "    x, labels = next(iter(loader))\n",
        "    x = x.to(device)\n",
        "    labels = labels.to(device)\n",
        "    logits = model(x)[:, -1]\n",
        "    return torch.nn.functional.cross_entropy(logits, labels)\n",
        "\n",
        "def full_accuracy(model, data, device):\n",
        "    loader = torch.utils.data.DataLoader(data, batch_size=len(data), shuffle=False)\n",
        "    # Take the final position only\n",
        "    x, labels = next(iter(loader))\n",
        "    x = x.to(device)\n",
        "    labels = labels.to(device)\n",
        "    logits = model(x)[:, -1]\n",
        "    predictions = torch.argmax(logits, dim=1)\n",
        "    return torch.sum(predictions == labels).item() / len(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KxaSLsKtsUK"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "JQnr1eRwJiXr"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "p = 113\n",
        "fraction = 0.5\n",
        "equals_token= p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "qzt0RjJuQmf9"
      },
      "outputs": [],
      "source": [
        "device='cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "uBbhwcOltKkC"
      },
      "outputs": [],
      "source": [
        "equals_token = p\n",
        "fraction=0.5\n",
        "x, y = torch.meshgrid(torch.arange(p), torch.arange(p), indexing='ij')\n",
        "x = x.flatten()\n",
        "y = y.flatten()\n",
        "equals = torch.ones(x.shape, dtype=torch.int64) * equals_token\n",
        "plus = torch.ones(x.shape, dtype=torch.int64) * (equals_token+1)\n",
        "prompts = torch.stack([x, plus, y, equals], dim=1).to(device)\n",
        "answers = ((x + y) % p).to(device)\n",
        "\n",
        "data = torch.utils.data.TensorDataset(prompts, answers)\n",
        "train, test = torch.utils.data.random_split(data,\n",
        "                                [int(fraction * len(data)),\n",
        "                                len(data) - int(fraction * len(data))\n",
        "                                ])\n",
        "equals_token = p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPEFz3X4txKk"
      },
      "source": [
        "# Training loop for modular addition (depth=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model declaration"
      ],
      "metadata": {
        "id": "6VuWcmB_zy5W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "LrWPliAqs0HV"
      },
      "outputs": [],
      "source": [
        "model = Transformer(num_layers=1,\n",
        "                    d_vocab=equals_token+3,\n",
        "                    d_model=128,\n",
        "                    d_mlp=512,\n",
        "                    d_head=128,\n",
        "                    num_heads=1,\n",
        "                    n_ctx=4, # context length\n",
        "                    act_type='ReLU',\n",
        "                    use_cache=False,\n",
        "                    use_ln=True, # use LayerNorm,\n",
        "                    depth=2\n",
        "                ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "kqe4tCvf8zIg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "bb9ee555ac7d474e98d3d5ee87ee376c",
            "d896a81851904403808ee9fda8edeffe",
            "b63537448347441a8386706c17ae5555",
            "f4032cf40ecf4182b970fae2dfe588a8",
            "84ed2ef4e608452d9d2ad0d9a5381022",
            "7b26742c7d774bed93893f544d58f431",
            "17e635e867c64dd084dd7e613ff742e8",
            "bdf7a358970a4ceca33ae3d4ffed20b7",
            "b099bc6706cf49299397025322dc6211",
            "9b2a3cb539b141c3a2d0b2e2e3de12d7",
            "84a93899823844228c75077deb8e4a41"
          ]
        },
        "id": "42T3-IZGVscY",
        "outputId": "2be934a9-8291-44b9-a226-ccbd7f9e80dc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb9ee555ac7d474e98d3d5ee87ee376c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0  | Train loss: 5.225423 |  Test loss: 5.240025 | train_acc: 0.01 | test_acc: 0.01 | l2: 45.650375\n",
            "epoch: 30  | Train loss: 4.677406 |  Test loss: 4.816740 | train_acc: 0.02 | test_acc: 0.00 | l2: 45.420148\n",
            "epoch: 60  | Train loss: 4.490737 |  Test loss: 5.428685 | train_acc: 0.04 | test_acc: 0.00 | l2: 46.007432\n",
            "epoch: 90  | Train loss: 4.261517 |  Test loss: 5.748648 | train_acc: 0.07 | test_acc: 0.00 | l2: 46.633254\n",
            "epoch: 120  | Train loss: 4.065578 |  Test loss: 5.745075 | train_acc: 0.08 | test_acc: 0.00 | l2: 47.996187\n",
            "epoch: 150  | Train loss: 3.928256 |  Test loss: 5.802274 | train_acc: 0.10 | test_acc: 0.00 | l2: 49.415690\n",
            "epoch: 180  | Train loss: 3.758621 |  Test loss: 5.775557 | train_acc: 0.13 | test_acc: 0.00 | l2: 50.549594\n",
            "epoch: 210  | Train loss: 3.333757 |  Test loss: 5.276145 | train_acc: 0.19 | test_acc: 0.01 | l2: 51.469159\n",
            "epoch: 240  | Train loss: 3.086468 |  Test loss: 5.052670 | train_acc: 0.22 | test_acc: 0.02 | l2: 51.908809\n",
            "epoch: 270  | Train loss: 2.951039 |  Test loss: 4.977964 | train_acc: 0.26 | test_acc: 0.02 | l2: 52.301408\n",
            "epoch: 300  | Train loss: 2.835070 |  Test loss: 4.909255 | train_acc: 0.28 | test_acc: 0.03 | l2: 52.684260\n",
            "epoch: 330  | Train loss: 2.671170 |  Test loss: 4.711219 | train_acc: 0.32 | test_acc: 0.04 | l2: 53.065168\n",
            "epoch: 360  | Train loss: 2.536196 |  Test loss: 4.541489 | train_acc: 0.35 | test_acc: 0.05 | l2: 53.462293\n",
            "epoch: 390  | Train loss: 2.403991 |  Test loss: 4.396451 | train_acc: 0.38 | test_acc: 0.06 | l2: 53.786778\n",
            "epoch: 420  | Train loss: 2.315825 |  Test loss: 4.308925 | train_acc: 0.40 | test_acc: 0.06 | l2: 54.070643\n",
            "epoch: 450  | Train loss: 2.238074 |  Test loss: 4.218639 | train_acc: 0.42 | test_acc: 0.07 | l2: 54.330304\n",
            "epoch: 480  | Train loss: 2.176761 |  Test loss: 4.153463 | train_acc: 0.43 | test_acc: 0.08 | l2: 54.615148\n",
            "epoch: 510  | Train loss: 2.136596 |  Test loss: 4.110157 | train_acc: 0.44 | test_acc: 0.09 | l2: 54.910866\n",
            "epoch: 540  | Train loss: 2.151968 |  Test loss: 4.139009 | train_acc: 0.42 | test_acc: 0.09 | l2: 55.160997\n",
            "epoch: 570  | Train loss: 2.001022 |  Test loss: 3.972954 | train_acc: 0.47 | test_acc: 0.10 | l2: 55.333000\n",
            "epoch: 600  | Train loss: 2.011968 |  Test loss: 3.934135 | train_acc: 0.47 | test_acc: 0.10 | l2: 55.539365\n",
            "epoch: 630  | Train loss: 1.895897 |  Test loss: 3.842299 | train_acc: 0.51 | test_acc: 0.11 | l2: 55.699105\n",
            "epoch: 660  | Train loss: 1.887514 |  Test loss: 3.746492 | train_acc: 0.50 | test_acc: 0.11 | l2: 55.813278\n",
            "epoch: 690  | Train loss: 1.873807 |  Test loss: 3.750947 | train_acc: 0.52 | test_acc: 0.12 | l2: 55.907464\n",
            "epoch: 720  | Train loss: 1.764462 |  Test loss: 3.617333 | train_acc: 0.56 | test_acc: 0.14 | l2: 56.040758\n",
            "epoch: 750  | Train loss: 1.780758 |  Test loss: 3.480580 | train_acc: 0.56 | test_acc: 0.14 | l2: 56.097849\n",
            "epoch: 780  | Train loss: 1.693249 |  Test loss: 3.462835 | train_acc: 0.59 | test_acc: 0.16 | l2: 56.126323\n",
            "epoch: 810  | Train loss: 1.681748 |  Test loss: 3.360799 | train_acc: 0.60 | test_acc: 0.16 | l2: 56.197937\n",
            "epoch: 840  | Train loss: 1.795681 |  Test loss: 3.432326 | train_acc: 0.55 | test_acc: 0.15 | l2: 56.337605\n",
            "epoch: 870  | Train loss: 1.612453 |  Test loss: 3.225978 | train_acc: 0.63 | test_acc: 0.18 | l2: 56.365016\n",
            "epoch: 900  | Train loss: 1.605125 |  Test loss: 3.208091 | train_acc: 0.63 | test_acc: 0.19 | l2: 56.477798\n",
            "epoch: 930  | Train loss: 1.801414 |  Test loss: 3.417011 | train_acc: 0.53 | test_acc: 0.16 | l2: 56.570445\n",
            "epoch: 960  | Train loss: 1.554011 |  Test loss: 3.055509 | train_acc: 0.65 | test_acc: 0.21 | l2: 56.538849\n",
            "epoch: 990  | Train loss: 1.901026 |  Test loss: 3.490359 | train_acc: 0.49 | test_acc: 0.15 | l2: 56.554825\n",
            "epoch: 1020  | Train loss: 1.514052 |  Test loss: 3.003272 | train_acc: 0.68 | test_acc: 0.22 | l2: 56.529862\n",
            "epoch: 1050  | Train loss: 1.550296 |  Test loss: 2.950803 | train_acc: 0.67 | test_acc: 0.22 | l2: 56.522554\n",
            "epoch: 1080  | Train loss: 1.465739 |  Test loss: 2.920450 | train_acc: 0.70 | test_acc: 0.24 | l2: 56.460341\n",
            "epoch: 1110  | Train loss: 1.471846 |  Test loss: 2.911983 | train_acc: 0.69 | test_acc: 0.23 | l2: 56.493549\n",
            "epoch: 1140  | Train loss: 1.701550 |  Test loss: 3.155123 | train_acc: 0.53 | test_acc: 0.17 | l2: 56.543118\n",
            "epoch: 1170  | Train loss: 1.399095 |  Test loss: 2.709873 | train_acc: 0.72 | test_acc: 0.27 | l2: 56.408611\n",
            "epoch: 1200  | Train loss: 1.311723 |  Test loss: 2.706379 | train_acc: 0.75 | test_acc: 0.29 | l2: 56.340100\n",
            "epoch: 1230  | Train loss: 1.347807 |  Test loss: 2.623053 | train_acc: 0.74 | test_acc: 0.28 | l2: 56.236122\n",
            "epoch: 1260  | Train loss: 1.913162 |  Test loss: 3.233655 | train_acc: 0.45 | test_acc: 0.17 | l2: 56.157274\n",
            "epoch: 1290  | Train loss: 1.285649 |  Test loss: 2.486653 | train_acc: 0.78 | test_acc: 0.31 | l2: 56.001508\n",
            "epoch: 1320  | Train loss: 1.325749 |  Test loss: 2.602689 | train_acc: 0.72 | test_acc: 0.28 | l2: 55.916291\n",
            "epoch: 1350  | Train loss: 1.217355 |  Test loss: 2.435493 | train_acc: 0.80 | test_acc: 0.33 | l2: 55.842430\n",
            "epoch: 1380  | Train loss: 1.260508 |  Test loss: 2.426489 | train_acc: 0.77 | test_acc: 0.31 | l2: 55.768365\n",
            "epoch: 1410  | Train loss: 1.266274 |  Test loss: 2.393012 | train_acc: 0.78 | test_acc: 0.32 | l2: 55.674166\n",
            "epoch: 1440  | Train loss: 1.182109 |  Test loss: 2.359475 | train_acc: 0.82 | test_acc: 0.35 | l2: 55.561224\n",
            "epoch: 1470  | Train loss: 1.215881 |  Test loss: 2.326882 | train_acc: 0.80 | test_acc: 0.35 | l2: 55.478183\n",
            "epoch: 1500  | Train loss: 1.267918 |  Test loss: 2.318799 | train_acc: 0.78 | test_acc: 0.34 | l2: 55.324591\n",
            "epoch: 1530  | Train loss: 1.168323 |  Test loss: 2.304285 | train_acc: 0.84 | test_acc: 0.37 | l2: 55.193888\n",
            "epoch: 1560  | Train loss: 1.242684 |  Test loss: 2.278879 | train_acc: 0.79 | test_acc: 0.35 | l2: 55.067516\n",
            "epoch: 1590  | Train loss: 1.245690 |  Test loss: 2.373007 | train_acc: 0.78 | test_acc: 0.33 | l2: 54.958947\n",
            "epoch: 1620  | Train loss: 1.193151 |  Test loss: 2.291396 | train_acc: 0.81 | test_acc: 0.36 | l2: 54.882412\n",
            "Epoch 01624: reducing learning rate of group 0 to 9.5000e-04.\n",
            "epoch: 1650  | Train loss: 1.232170 |  Test loss: 2.279332 | train_acc: 0.80 | test_acc: 0.35 | l2: 54.809605\n",
            "epoch: 1680  | Train loss: 1.414487 |  Test loss: 2.446639 | train_acc: 0.68 | test_acc: 0.30 | l2: 54.750585\n",
            "epoch: 1710  | Train loss: 1.167802 |  Test loss: 2.219310 | train_acc: 0.84 | test_acc: 0.38 | l2: 54.626950\n",
            "epoch: 1740  | Train loss: 1.386383 |  Test loss: 2.302739 | train_acc: 0.72 | test_acc: 0.32 | l2: 54.505726\n",
            "epoch: 1770  | Train loss: 1.201148 |  Test loss: 2.167143 | train_acc: 0.84 | test_acc: 0.38 | l2: 54.297901\n",
            "epoch: 1800  | Train loss: 1.827879 |  Test loss: 2.741889 | train_acc: 0.52 | test_acc: 0.23 | l2: 54.180657\n",
            "Epoch 01811: reducing learning rate of group 0 to 9.0250e-04.\n",
            "epoch: 1830  | Train loss: 1.230603 |  Test loss: 2.158326 | train_acc: 0.82 | test_acc: 0.38 | l2: 53.995997\n",
            "epoch: 1860  | Train loss: 1.342005 |  Test loss: 2.270282 | train_acc: 0.75 | test_acc: 0.34 | l2: 53.902562\n",
            "epoch: 1890  | Train loss: 1.217589 |  Test loss: 2.193692 | train_acc: 0.83 | test_acc: 0.38 | l2: 53.835617\n",
            "Epoch 01902: reducing learning rate of group 0 to 8.5737e-04.\n",
            "epoch: 1920  | Train loss: 1.228564 |  Test loss: 2.195740 | train_acc: 0.82 | test_acc: 0.38 | l2: 53.822667\n",
            "epoch: 1950  | Train loss: 1.516481 |  Test loss: 2.410607 | train_acc: 0.65 | test_acc: 0.29 | l2: 53.794993\n",
            "epoch: 1980  | Train loss: 1.215258 |  Test loss: 2.144124 | train_acc: 0.84 | test_acc: 0.39 | l2: 53.681759\n",
            "Epoch 01993: reducing learning rate of group 0 to 8.1451e-04.\n",
            "epoch: 2010  | Train loss: 1.836848 |  Test loss: 2.705786 | train_acc: 0.54 | test_acc: 0.25 | l2: 53.661013\n",
            "epoch: 2040  | Train loss: 1.247034 |  Test loss: 2.123690 | train_acc: 0.83 | test_acc: 0.39 | l2: 53.501097\n",
            "epoch: 2070  | Train loss: 1.894612 |  Test loss: 2.853636 | train_acc: 0.51 | test_acc: 0.23 | l2: 53.440746\n",
            "Epoch 02084: reducing learning rate of group 0 to 7.7378e-04.\n",
            "epoch: 2100  | Train loss: 1.244682 |  Test loss: 2.136299 | train_acc: 0.83 | test_acc: 0.39 | l2: 53.372517\n",
            "epoch: 2130  | Train loss: 1.703201 |  Test loss: 2.657289 | train_acc: 0.58 | test_acc: 0.26 | l2: 53.407263\n",
            "epoch: 2160  | Train loss: 1.315687 |  Test loss: 2.161963 | train_acc: 0.80 | test_acc: 0.38 | l2: 53.291052\n",
            "Epoch 02175: reducing learning rate of group 0 to 7.3509e-04.\n",
            "epoch: 2190  | Train loss: 1.219599 |  Test loss: 2.141868 | train_acc: 0.83 | test_acc: 0.39 | l2: 53.288557\n",
            "epoch: 2220  | Train loss: 1.325353 |  Test loss: 2.193443 | train_acc: 0.77 | test_acc: 0.36 | l2: 53.264171\n",
            "epoch: 2250  | Train loss: 1.254877 |  Test loss: 2.167192 | train_acc: 0.82 | test_acc: 0.38 | l2: 53.219615\n",
            "Epoch 02266: reducing learning rate of group 0 to 6.9834e-04.\n",
            "epoch: 2280  | Train loss: 1.317451 |  Test loss: 2.148316 | train_acc: 0.79 | test_acc: 0.37 | l2: 53.134153\n",
            "epoch: 2310  | Train loss: 1.226942 |  Test loss: 2.126046 | train_acc: 0.84 | test_acc: 0.39 | l2: 53.120450\n",
            "epoch: 2340  | Train loss: 1.492544 |  Test loss: 2.387365 | train_acc: 0.66 | test_acc: 0.31 | l2: 53.171145\n",
            "Epoch 02357: reducing learning rate of group 0 to 6.6342e-04.\n",
            "epoch: 2370  | Train loss: 1.241665 |  Test loss: 2.121250 | train_acc: 0.82 | test_acc: 0.39 | l2: 53.152463\n",
            "epoch: 2400  | Train loss: 1.656752 |  Test loss: 2.555089 | train_acc: 0.58 | test_acc: 0.27 | l2: 53.189680\n",
            "epoch: 2430  | Train loss: 1.240961 |  Test loss: 2.122903 | train_acc: 0.82 | test_acc: 0.39 | l2: 53.199757\n",
            "Epoch 02448: reducing learning rate of group 0 to 6.3025e-04.\n",
            "epoch: 2460  | Train loss: 1.572733 |  Test loss: 2.471397 | train_acc: 0.61 | test_acc: 0.27 | l2: 53.239532\n",
            "epoch: 2490  | Train loss: 1.233338 |  Test loss: 2.107789 | train_acc: 0.82 | test_acc: 0.39 | l2: 53.229402\n",
            "epoch: 2520  | Train loss: 1.317807 |  Test loss: 2.242573 | train_acc: 0.75 | test_acc: 0.35 | l2: 53.297245\n",
            "Epoch 02539: reducing learning rate of group 0 to 5.9874e-04.\n",
            "epoch: 2550  | Train loss: 1.306006 |  Test loss: 2.206279 | train_acc: 0.76 | test_acc: 0.35 | l2: 53.342417\n",
            "epoch: 2580  | Train loss: 1.212779 |  Test loss: 2.121930 | train_acc: 0.83 | test_acc: 0.39 | l2: 53.387024\n",
            "epoch: 2610  | Train loss: 1.246654 |  Test loss: 2.123805 | train_acc: 0.80 | test_acc: 0.38 | l2: 53.403563\n",
            "Epoch 02630: reducing learning rate of group 0 to 5.6880e-04.\n",
            "epoch: 2640  | Train loss: 1.253994 |  Test loss: 2.165834 | train_acc: 0.80 | test_acc: 0.38 | l2: 53.460134\n",
            "epoch: 2670  | Train loss: 1.299036 |  Test loss: 2.171932 | train_acc: 0.77 | test_acc: 0.36 | l2: 53.470966\n",
            "epoch: 2700  | Train loss: 1.182119 |  Test loss: 2.080816 | train_acc: 0.84 | test_acc: 0.41 | l2: 53.494930\n",
            "Epoch 02721: reducing learning rate of group 0 to 5.4036e-04.\n",
            "epoch: 2730  | Train loss: 1.366509 |  Test loss: 2.210223 | train_acc: 0.72 | test_acc: 0.34 | l2: 53.485621\n",
            "epoch: 2760  | Train loss: 1.197862 |  Test loss: 2.057339 | train_acc: 0.84 | test_acc: 0.41 | l2: 53.447136\n",
            "epoch: 2790  | Train loss: 2.145846 |  Test loss: 3.072475 | train_acc: 0.47 | test_acc: 0.23 | l2: 53.495207\n",
            "Epoch 02812: reducing learning rate of group 0 to 5.1334e-04.\n",
            "epoch: 2820  | Train loss: 1.239522 |  Test loss: 2.091120 | train_acc: 0.80 | test_acc: 0.39 | l2: 53.442951\n",
            "epoch: 2850  | Train loss: 1.177758 |  Test loss: 2.064878 | train_acc: 0.85 | test_acc: 0.42 | l2: 53.470397\n",
            "epoch: 2880  | Train loss: 1.237915 |  Test loss: 2.100672 | train_acc: 0.81 | test_acc: 0.39 | l2: 53.486855\n",
            "Epoch 02903: reducing learning rate of group 0 to 4.8767e-04.\n",
            "epoch: 2910  | Train loss: 1.163934 |  Test loss: 2.055413 | train_acc: 0.85 | test_acc: 0.42 | l2: 53.542834\n",
            "epoch: 2940  | Train loss: 1.227721 |  Test loss: 2.098620 | train_acc: 0.81 | test_acc: 0.39 | l2: 53.580915\n",
            "epoch: 2970  | Train loss: 1.231611 |  Test loss: 2.124652 | train_acc: 0.79 | test_acc: 0.38 | l2: 53.618965\n",
            "Epoch 02994: reducing learning rate of group 0 to 4.6329e-04.\n",
            "epoch: 3000  | Train loss: 1.193538 |  Test loss: 2.056360 | train_acc: 0.82 | test_acc: 0.41 | l2: 53.647374\n",
            "epoch: 3030  | Train loss: 1.154552 |  Test loss: 2.042908 | train_acc: 0.85 | test_acc: 0.42 | l2: 53.729937\n",
            "epoch: 3060  | Train loss: 1.252236 |  Test loss: 2.094599 | train_acc: 0.78 | test_acc: 0.38 | l2: 53.739765\n",
            "epoch: 3090  | Train loss: 1.135184 |  Test loss: 2.004518 | train_acc: 0.86 | test_acc: 0.43 | l2: 53.758579\n",
            "epoch: 3120  | Train loss: 1.493667 |  Test loss: 2.382654 | train_acc: 0.65 | test_acc: 0.31 | l2: 53.809584\n",
            "epoch: 3150  | Train loss: 1.139962 |  Test loss: 1.978591 | train_acc: 0.86 | test_acc: 0.44 | l2: 53.770944\n",
            "epoch: 3180  | Train loss: 1.574260 |  Test loss: 2.435187 | train_acc: 0.61 | test_acc: 0.30 | l2: 53.792313\n",
            "epoch: 3210  | Train loss: 1.135611 |  Test loss: 1.972786 | train_acc: 0.86 | test_acc: 0.44 | l2: 53.794251\n",
            "epoch: 3240  | Train loss: 1.410075 |  Test loss: 2.240374 | train_acc: 0.68 | test_acc: 0.34 | l2: 53.840291\n",
            "epoch: 3270  | Train loss: 1.108849 |  Test loss: 1.938625 | train_acc: 0.87 | test_acc: 0.45 | l2: 53.858232\n",
            "epoch: 3300  | Train loss: 1.312495 |  Test loss: 2.112056 | train_acc: 0.74 | test_acc: 0.38 | l2: 53.817122\n",
            "epoch: 3330  | Train loss: 1.114222 |  Test loss: 1.914427 | train_acc: 0.87 | test_acc: 0.45 | l2: 53.776078\n",
            "epoch: 3360  | Train loss: 1.372576 |  Test loss: 2.183651 | train_acc: 0.69 | test_acc: 0.35 | l2: 53.790008\n",
            "epoch: 3390  | Train loss: 1.101275 |  Test loss: 1.890952 | train_acc: 0.87 | test_acc: 0.45 | l2: 53.776041\n",
            "epoch: 3420  | Train loss: 1.240014 |  Test loss: 2.037004 | train_acc: 0.78 | test_acc: 0.39 | l2: 53.786845\n",
            "epoch: 3450  | Train loss: 1.096169 |  Test loss: 1.892783 | train_acc: 0.87 | test_acc: 0.45 | l2: 53.792993\n",
            "epoch: 3480  | Train loss: 1.128597 |  Test loss: 1.914220 | train_acc: 0.84 | test_acc: 0.44 | l2: 53.821303\n",
            "epoch: 3510  | Train loss: 1.340480 |  Test loss: 2.150513 | train_acc: 0.71 | test_acc: 0.36 | l2: 53.849416\n",
            "epoch: 3540  | Train loss: 1.076182 |  Test loss: 1.852057 | train_acc: 0.87 | test_acc: 0.46 | l2: 53.861881\n",
            "epoch: 3570  | Train loss: 1.273155 |  Test loss: 2.031514 | train_acc: 0.74 | test_acc: 0.38 | l2: 53.849223\n",
            "epoch: 3600  | Train loss: 1.075519 |  Test loss: 1.832750 | train_acc: 0.88 | test_acc: 0.46 | l2: 53.850656\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1.0, betas=(0.9, 0.98))\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.95,patience=90, threshold=10e-8,verbose=True)\n",
        "log_steps = []\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "test_minus= []\n",
        "norms = []\n",
        "Attn_wk_m=[]\n",
        "Attn_wq_m=[]\n",
        "Attn_wv_m=[]\n",
        "for epoch in tqdm(range(30000)):\n",
        "  train_loss = full_loss(model, train, device)\n",
        "  scheduler.step(train_loss)\n",
        "  if epoch % 30 == 0:\n",
        "     for param_tensor in model.state_dict():\n",
        "          if param_tensor=='blocks.0.attn.key_matrix.weight':\n",
        "            Attn_wk_m.append(model.state_dict()[param_tensor].cpu().detach().numpy())\n",
        "          if param_tensor=='blocks.0.attn.query_matrix.weight':\n",
        "            Attn_wq_m.append(model.state_dict()[param_tensor].cpu().detach().numpy())\n",
        "          if param_tensor=='blocks.0.attn.value_matrix.weight':\n",
        "            Attn_wv_m.append(model.state_dict()[param_tensor].cpu().detach().numpy())\n",
        "     with torch.no_grad():\n",
        "          log_steps.append(epoch)\n",
        "          test_loss = full_loss(model, test, device)\n",
        "          test_2,train_2=test_loss.item(),train_loss.item()\n",
        "          train_losses.append(train_loss.item())\n",
        "          test_losses.append(test_loss.item())\n",
        "          train_accuracies.append(full_accuracy(model, train, device))\n",
        "          test_accuracies.append(full_accuracy(model, test, device))\n",
        "          norms.append(np.sqrt(sum(param.pow(2).sum().item() for param in model.parameters())))\n",
        "          print(\"epoch: %d  | Train loss: %.6f |  Test loss: %.6f | train_acc: %.2f | test_acc: %.2f | l2: %.6f\"%(epoch, train_losses[-1], test_losses[-1], train_accuracies[-1], test_accuracies[-1], norms[-1]))\n",
        "  train_loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  # Print model's state_dict# print(\"Model's state_dict:\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPoaGh2Ut2Kk"
      },
      "source": [
        "## Vizualization of the accuracy, rank and weight models dynamics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure\n",
        "plt.plot(log_steps,train_losses, color=Z[1],label='Train losses')\n",
        "plt.plot(log_steps,test_losses, color=Z[7],label='Test losses')\n",
        "plt.legend(fontsize='large')\n",
        "plt.xlabel('Number Of Epochs')\n",
        "plt.ylabel('Train/Test Losses')\n",
        "plt.xscale('log')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U-0Vkb0SWeri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure\n",
        "plt.plot(log_steps,train_accuracies , color=Z[1],label='Train Accuracies')\n",
        "plt.plot(log_steps,test_accuracies, color=Z[7],label='Test Accuracies')\n",
        "plt.legend(fontsize='large')\n",
        "plt.xlabel('Number Of Epochs')\n",
        "plt.ylabel('Train/Test Accuracies')\n",
        "plt.xscale('log')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "blEcVe1KnSP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank_wk=[np.linalg.matrix_rank(x) for x in Attn_wk_m]\n",
        "rank_wq_m=[np.linalg.matrix_rank(x) for x in Attn_wq_m]\n",
        "rank_wv_m=[np.linalg.matrix_rank(x) for x in Attn_wv_m]"
      ],
      "metadata": {
        "id": "qME2tyY6j86f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rk_wk=[x/128 for x in rank_wk]\n",
        "rk_wq=[x/128 for x in rank_wq_m]\n",
        "rk_wv=[x/128 for x in rank_wv_m]"
      ],
      "metadata": {
        "id": "rB2ss7GIldEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eigenvalues, eigenvectors = np.linalg.eig(Attn_wv_m[-1])"
      ],
      "metadata": {
        "id": "1YOsWllJonXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax=plt.subplots(nrows=1,ncols=2,dpi=100,figsize=[35,8])\n",
        "\n",
        "ax[0].plot(log_steps, rk_wk, color=Z[0], linewidth=3, label='W_K rank',zorder=4)\n",
        "ax[0].plot(log_steps, rk_wv, color=Z[3],linewidth=3,label='W_V rank',zorder=1)\n",
        "ax[0].plot(log_steps, rk_wq, color=Z[1],linewidth=3, label='W_Q rank',zorder=3)\n",
        "ax[0].plot(log_steps, train_accuracies, color=Z[6],linewidth=3, label='Train accuracies',zorder=3)\n",
        "ax[0].plot(log_steps, test_accuracies, color=Z[7],linewidth=3, label='Test accuracies',zorder=3)\n",
        "ax[0].legend()\n",
        "ax[0].set_ylabel('Rank of attention matrices',fontsize='large')\n",
        "ax[0].set_xlabel(\"Number of epochs\",fontsize='large')\n",
        "ax[0].set_xlim(1, 3*10**4)\n",
        "ax[0].set_xscale('log')\n",
        "ax[0].legend(fontsize='large')\n",
        "#ax[1].set_title(\"Rank dynamics of attention matrices in the second layer\".capitalize(),fontsize='large')\n",
        "\n",
        "ax[1].plot(log_steps, train_losses, color=Z[5],linewidth=3, label='Train loss',zorder=3)\n",
        "ax[1].plot(log_steps, test_losses, color=Z[4],linewidth=3, label='Test loss',zorder=3)\n",
        "#ax[2].plot(log_steps, rk_wk, color=Z[0], linewidth=3, label='W_K rank',zorder=4)\n",
        "#ax[2].plot(log_steps, rk_wv, color=Z[3],linewidth=3,label='W_V rank',zorder=1)\n",
        "#ax[2].plot(log_steps, rk_wq, color=Z[1],linewidth=3, label='W_Q rank',zorder=3)\n",
        "ax[1].legend()\n",
        "ax[1].set_ylabel('Train/Test Losses',fontsize='large')\n",
        "ax[1].set_xlabel(\"Number of epochs\",fontsize='large')\n",
        "ax[1].set_xlim(1, 3*10**4)\n",
        "ax[1].set_xscale('log')\n",
        "##ax[2].set_yscale('log')\n",
        "ax[1].legend(fontsize='large')\n",
        "#ax[2].set_title(\"Rank dynamics of attention matrices in the second layer\".capitalize(),fontsize='large')"
      ],
      "metadata": {
        "id": "v9Zw3LrYpcS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure\n",
        "plt.scatter(np.real(eigenvalues), np.imag(eigenvalues), marker='o', color=Z[1])\n",
        "plt.axhline(0, color='black',linewidth=0.01)\n",
        "plt.axvline(0, color='black',linewidth=0.01)\n",
        "plt.grid(color = 'gray', linestyle = '--', linewidth = 0.01)\n",
        "plt.xlabel('Spectrum of the Values Matrix')\n",
        "plt.legend()\n",
        "plt.legend(fontsize='large')"
      ],
      "metadata": {
        "id": "kMn6mx6v6aPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueM5rex99Ngp"
      },
      "source": [
        "# Training loop for modular addition (depth=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model declaration"
      ],
      "metadata": {
        "id": "XEt4fksG9Ngy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1dUFwNA9Ngy"
      },
      "outputs": [],
      "source": [
        "model_4 = Transformer(num_layers=1,\n",
        "                    d_vocab=equals_token+3,\n",
        "                    d_model=128,\n",
        "                    d_mlp=512,\n",
        "                    d_head=128,\n",
        "                    num_heads=1,\n",
        "                    n_ctx=4, # context length\n",
        "                    act_type='ReLU',\n",
        "                    use_cache=False,\n",
        "                    use_ln=True, # use LayerNorm,\n",
        "                    depth=4\n",
        "                ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "d0-r7w_R9Ngy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shtAiWdO9Ngz"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model_4.parameters(), lr=1e-3, weight_decay=1.0, betas=(0.9, 0.98))\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.95,patience=100, threshold=10e-8,verbose=True)\n",
        "log_steps_4 = []\n",
        "train_losses_4 = []\n",
        "test_losses_4 = []\n",
        "train_accuracies_4 = []\n",
        "test_accuracies_4 = []\n",
        "test_minus_4= []\n",
        "norms_4 = []\n",
        "Attn_wk_m_4=[]\n",
        "Attn_wq_m_4=[]\n",
        "Attn_wv_m_4=[]\n",
        "for epoch in tqdm(range(50000)):\n",
        "  train_loss = full_loss(model_4, train, device)\n",
        "  scheduler.step(train_loss)\n",
        "  if epoch % 30 == 0:\n",
        "     for param_tensor in model_4.state_dict():\n",
        "          if param_tensor=='blocks.0.attn.key_matrix.weight':\n",
        "            Attn_wk_m_4.append(model_4.state_dict()[param_tensor].cpu().detach().numpy())\n",
        "          if param_tensor=='blocks.0.attn.query_matrix.weight':\n",
        "            Attn_wq_m_4.append(model_4.state_dict()[param_tensor].cpu().detach().numpy())\n",
        "          if param_tensor=='blocks.0.attn.value_matrix.weight':\n",
        "            Attn_wv_m_4.append(model_4.state_dict()[param_tensor].cpu().detach().numpy())\n",
        "     with torch.no_grad():\n",
        "          log_steps_4.append(epoch)\n",
        "          test_loss = full_loss(model_4, test, device)\n",
        "          test_2,train_2=test_loss.item(),train_loss.item()\n",
        "          train_losses_4.append(train_loss.item())\n",
        "          test_losses_4.append(test_loss.item())\n",
        "          train_accuracies_4.append(full_accuracy(model_4, train, device))\n",
        "          test_accuracies_4.append(full_accuracy(model_4, test, device))\n",
        "          norms_4.append(np.sqrt(sum(param.pow(2).sum().item() for param in model_4.parameters())))\n",
        "          print(\"epoch: %d  | Train loss: %.6f |  Test loss: %.6f | train_acc: %.2f | test_acc: %.2f | l2: %.6f\"%(epoch, train_losses_4[-1], test_losses_4[-1], train_accuracies_4[-1], test_accuracies_4[-1], norms_4[-1]))\n",
        "  train_loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure\n",
        "plt.plot(log_steps_4,train_losses_4, color=Z[1],label='Train losses')\n",
        "plt.plot(log_steps_4,test_losses_4, color=Z[7],label='Test losses')\n",
        "plt.legend(fontsize='large')\n",
        "plt.xlabel('Number Of Epochs')\n",
        "#plt.ylabel('Train/Test Accuracies')\n",
        "plt.ylabel('Train/Test Losses')\n",
        "plt.xscale('log')\n",
        "plt.xlim(1, 5*10**4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "krqGAX5q9Ngz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure\n",
        "plt.plot(log_steps_4,train_accuracies_4, color=Z[1],label='Train accuracies')\n",
        "plt.plot(log_steps_4,test_accuracies_4, color=Z[7],label='Test accuracies')\n",
        "plt.ylabel('Train/Test Accuracies')\n",
        "plt.xlabel('Number Of Epochs')\n",
        "plt.xscale('log')\n",
        "plt.xlim(1, 5*10**4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0_BiPIuYN3zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank_wk_4=[np.linalg.matrix_rank(x) for x in Attn_wk_m_4]\n",
        "rank_wq_m_4=[np.linalg.matrix_rank(x) for x in Attn_wq_m_4]\n",
        "rank_wv_m_4=[np.linalg.matrix_rank(x) for x in Attn_wv_m_4]"
      ],
      "metadata": {
        "id": "mv_cQNh39Ngz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rk_wk_4=[x/128 for x in rank_wk_4]\n",
        "rk_wq_4=[x/128 for x in rank_wq_m_4]\n",
        "rk_wv_4=[x/128 for x in rank_wv_m_4]"
      ],
      "metadata": {
        "id": "zm4DQVlD9Ng0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eigenvalues_4, eigenvectors_4 = np.linalg.eig(Attn_wv_m_4[-1])"
      ],
      "metadata": {
        "id": "EPNoRZEZ9Ng0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax=plt.subplots(nrows=1,ncols=2,dpi=100,figsize=[15,8])\n",
        "\n",
        "ax[0].plot(log_steps_4, rk_wk_4, color=Z[0], linewidth=3, label='W_K rank',zorder=4)\n",
        "ax[0].plot(log_steps_4, rk_wv_4, color=Z[3],linewidth=3,label='W_V rank',zorder=1)\n",
        "ax[0].plot(log_steps_4, rk_wq_4, color=Z[1],linewidth=3, label='W_Q rank',zorder=3)\n",
        "ax[0].plot(log_steps_4, train_accuracies_4, color=Z[6],linewidth=3, label='Train accuracies',zorder=3)\n",
        "ax[0].plot(log_steps_4, test_accuracies_4, color=Z[7],linewidth=3, label='Test accuracies',zorder=3)\n",
        "ax[0].legend()\n",
        "ax[0].set_ylabel('Train/Test Accuracies And Rank of attention matrices',fontsize='large')\n",
        "ax[0].set_xlabel(\"Number of epochs\",fontsize='large')\n",
        "ax[0].set_xlim(1, 5*10**4)\n",
        "ax[0].set_xscale('log')\n",
        "ax[0].legend(fontsize='large')\n",
        "#ax[1].set_title(\"Rank dynamics of attention matrices in the second layer\".capitalize(),fontsize='large')\n",
        "\n",
        "ax[1].plot(log_steps_4, train_losses_4, color=Z[5],linewidth=3, label='Train loss',zorder=3)\n",
        "ax[1].plot(log_steps_4, test_losses_4, color=Z[4],linewidth=3, label='Test loss',zorder=3)\n",
        "#ax[2].plot(log_steps, rk_wk, color=Z[0], linewidth=3, label='W_K rank',zorder=4)\n",
        "#ax[2].plot(log_steps, rk_wv, color=Z[3],linewidth=3,label='W_V rank',zorder=1)\n",
        "#ax[2].plot(log_steps, rk_wq, color=Z[1],linewidth=3, label='W_Q rank',zorder=3)\n",
        "ax[1].legend()\n",
        "ax[1].set_ylabel('Train/Test Losses',fontsize='large')\n",
        "ax[1].set_xlabel(\"Number of epochs\",fontsize='large')\n",
        "ax[1].set_xlim(1, 5*10**4)\n",
        "ax[1].set_xscale('log')\n",
        "##ax[2].set_yscale('log')\n",
        "ax[1].legend(fontsize='large')\n",
        "#ax[2].set_title(\"Rank dynamics of attention matrices in the second layer\".capitalize(),fontsize='large')"
      ],
      "metadata": {
        "id": "KWPPmrKv9Ng0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure\n",
        "plt.scatter(np.real(eigenvalues_4), np.imag(eigenvalues_4), marker='o', color=Z[1])\n",
        "plt.axhline(0, color='black',linewidth=0.01)\n",
        "plt.axvline(0, color='black',linewidth=0.01)\n",
        "plt.grid(color = 'gray', linestyle = '--', linewidth = 0.01)\n",
        "plt.xlabel('Eigenvalues of V')\n",
        "plt.legend()\n",
        "plt.legend(fontsize='large')"
      ],
      "metadata": {
        "id": "PdL4_DIS9Ng0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AynG0ha1Ur4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMsv3OsnoXn4"
      },
      "source": [
        "# Training loop for modular addition (depth=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model declaration"
      ],
      "metadata": {
        "id": "WUAEQK5moXoA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYLBCRmVoXoA"
      },
      "outputs": [],
      "source": [
        "model_6 = Transformer(num_layers=1,\n",
        "                    d_vocab=equals_token+3,\n",
        "                    d_model=128,\n",
        "                    d_mlp=512,\n",
        "                    d_head=128,\n",
        "                    num_heads=1,\n",
        "                    n_ctx=4, # context length\n",
        "                    act_type='ReLU',\n",
        "                    use_cache=False,\n",
        "                    use_ln=True, # use LayerNorm,\n",
        "                    depth=6\n",
        "                ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "sDvOMxO1oXoA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxsfpigpoXoA"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1.0, betas=(0.9, 0.98))\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.95,patience=90, threshold=10e-8,verbose=True)\n",
        "log_steps_6 = []\n",
        "train_losses_6 = []\n",
        "test_losses_6 = []\n",
        "train_accuracies_6 = []\n",
        "test_accuracies_6 = []\n",
        "norms_6 = []\n",
        "Attn_wk_m_6=[]\n",
        "Attn_wq_m_6=[]\n",
        "Attn_wv_m_6=[]\n",
        "for epoch in tqdm(range(30000)):\n",
        "  train_loss = full_loss(model_6, train, device)\n",
        "  scheduler.step(train_loss)\n",
        "  if epoch % 30 == 0:\n",
        "     for param_tensor in model_6.state_dict():\n",
        "          if param_tensor=='blocks.0.attn.key_matrix.weight':\n",
        "            Attn_wk_m_6.append(model_6.state_dict()[param_tensor].cpu().detach().numpy())\n",
        "          if param_tensor=='blocks.0.attn.query_matrix.weight':\n",
        "            Attn_wq_m_6.append(model_6.state_dict()[param_tensor].cpu().detach().numpy())\n",
        "          if param_tensor=='blocks.0.attn.value_matrix.weight':\n",
        "            Attn_wv_m_6.append(model_6.state_dict()[param_tensor].cpu().detach().numpy())\n",
        "     with torch.no_grad():\n",
        "          log_steps_6.append(epoch)\n",
        "          test_minus_6.append(full_accuracy(model_6,datatest_minus, device))\n",
        "          test_loss_6 = full_loss(model_6, test, device)\n",
        "          test_2,train_2=test_loss.item(),train_loss.item()\n",
        "          train_losses_6.append(train_loss.item())\n",
        "          test_losses_6.append(test_loss.item())\n",
        "          train_accuracies_6.append(full_accuracy(model_6, train, device))\n",
        "          test_accuracies_6.append(full_accuracy(model_6, test, device))\n",
        "          norms_6.append(np.sqrt(sum(param.pow(2).sum().item() for param in model_6.parameters())))\n",
        "          print(\"epoch: %d  | Train loss: %.6f |  Test loss: %.6f | train_acc: %.2f | test_acc: %.2f | l2: %.6f\"%(epoch, train_losses_6[-1], test_losses_6[-1], train_accuracies_6[-1], test_accuracies_6[-1], norms_6[-1]))\n",
        "  train_loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  # Print model's state_dict# print(\"Model's state_dict:\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuFwgjWroXoB"
      },
      "source": [
        "## Vizualization of the accuracy, rank and weight models dynamics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure\n",
        "plt.plot(log_steps_6,train_losses_6, color=Z[1],label='Train losses')\n",
        "plt.plot(log_steps_6,test_losses_6, color=Z[7],label='Test losses')\n",
        "plt.legend(fontsize='large')\n",
        "plt.xlabel('Number Of Epochs')\n",
        "plt.ylabel('Train/Test Losses')\n",
        "plt.xscale('log')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JmHuHD6WoXoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure\n",
        "plt.plot(log_steps,train_accuracies , color=Z[1],label='Train Accuracies')\n",
        "plt.plot(log_steps,test_accuracies, color=Z[7],label='Test Accuracies')\n",
        "plt.legend(fontsize='large')\n",
        "plt.xlabel('Number Of Epochs')\n",
        "plt.ylabel('Train/Test Accuracies')\n",
        "plt.xscale('log')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g-MKod-3oXoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank_wk=[np.linalg.matrix_rank(x) for x in Attn_wk_m]\n",
        "rank_wq_m=[np.linalg.matrix_rank(x) for x in Attn_wq_m]\n",
        "rank_wv_m=[np.linalg.matrix_rank(x) for x in Attn_wv_m]"
      ],
      "metadata": {
        "id": "UWprPpzPoXoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rk_wk=[x/128 for x in rank_wk]\n",
        "rk_wq=[x/128 for x in rank_wq_m]\n",
        "rk_wv=[x/128 for x in rank_wv_m]"
      ],
      "metadata": {
        "id": "9nkS7zMVoXoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eigenvalues, eigenvectors = np.linalg.eig(Attn_wv_m[-1])"
      ],
      "metadata": {
        "id": "NY3JIErKoXoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax=plt.subplots(nrows=1,ncols=2,dpi=100,figsize=[35,8])\n",
        "\n",
        "ax[0].plot(log_steps, rk_wk, color=Z[0], linewidth=3, label='W_K rank',zorder=4)\n",
        "ax[0].plot(log_steps, rk_wv, color=Z[3],linewidth=3,label='W_V rank',zorder=1)\n",
        "ax[0].plot(log_steps, rk_wq, color=Z[1],linewidth=3, label='W_Q rank',zorder=3)\n",
        "ax[0].plot(log_steps, train_accuracies, color=Z[6],linewidth=3, label='Train accuracies',zorder=3)\n",
        "ax[0].plot(log_steps, test_accuracies, color=Z[7],linewidth=3, label='Test accuracies',zorder=3)\n",
        "ax[0].legend()\n",
        "ax[0].set_ylabel('Rank of attention matrices',fontsize='large')\n",
        "ax[0].set_xlabel(\"Number of epochs\",fontsize='large')\n",
        "ax[0].set_xlim(1, 3*10**4)\n",
        "ax[0].set_xscale('log')\n",
        "ax[0].legend(fontsize='large')\n",
        "#ax[1].set_title(\"Rank dynamics of attention matrices in the second layer\".capitalize(),fontsize='large')\n",
        "\n",
        "ax[1].plot(log_steps, train_losses, color=Z[5],linewidth=3, label='Train loss',zorder=3)\n",
        "ax[1].plot(log_steps, test_losses, color=Z[4],linewidth=3, label='Test loss',zorder=3)\n",
        "#ax[2].plot(log_steps, rk_wk, color=Z[0], linewidth=3, label='W_K rank',zorder=4)\n",
        "#ax[2].plot(log_steps, rk_wv, color=Z[3],linewidth=3,label='W_V rank',zorder=1)\n",
        "#ax[2].plot(log_steps, rk_wq, color=Z[1],linewidth=3, label='W_Q rank',zorder=3)\n",
        "ax[1].legend()\n",
        "ax[1].set_ylabel('Train/Test Losses',fontsize='large')\n",
        "ax[1].set_xlabel(\"Number of epochs\",fontsize='large')\n",
        "ax[1].set_xlim(1, 3*10**4)\n",
        "ax[1].set_xscale('log')\n",
        "##ax[2].set_yscale('log')\n",
        "ax[1].legend(fontsize='large')\n",
        "#ax[2].set_title(\"Rank dynamics of attention matrices in the second layer\".capitalize(),fontsize='large')"
      ],
      "metadata": {
        "id": "I_ifEHKgoXoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure\n",
        "plt.scatter(np.real(eigenvalues), np.imag(eigenvalues), marker='o', color=Z[1])\n",
        "plt.axhline(0, color='black',linewidth=0.01)\n",
        "plt.axvline(0, color='black',linewidth=0.01)\n",
        "plt.grid(color = 'gray', linestyle = '--', linewidth = 0.01)\n",
        "plt.xlabel('Spectrum of the Values Matrix')\n",
        "plt.legend()\n",
        "plt.legend(fontsize='large')"
      ],
      "metadata": {
        "id": "_7KORl4NoXoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KzBFRamUzBk"
      },
      "source": [
        "# Training loop for modular addition (depth=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model declaration"
      ],
      "metadata": {
        "id": "GUtdShTSUzBt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL5AahlCUzBt"
      },
      "outputs": [],
      "source": [
        "model_8 = Transformer(num_layers=1,\n",
        "                    d_vocab=equals_token+3,\n",
        "                    d_model=128,\n",
        "                    d_mlp=512,\n",
        "                    d_head=128,\n",
        "                    num_heads=1,\n",
        "                    n_ctx=4, # context length\n",
        "                    act_type='ReLU',\n",
        "                    use_cache=False,\n",
        "                    use_ln=True, # use LayerNorm,\n",
        "                    depth=8\n",
        "                ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "yv1DSP9QUzBt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dgwDM-CUzBt"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model_8.parameters(), lr=1e-3, weight_decay=1.0, betas=(0.9, 0.98))\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.95,patience=100, threshold=10e-8,verbose=True)\n",
        "log_steps_8 = []\n",
        "train_losses_8 = []\n",
        "test_losses_8 = []\n",
        "train_accuracies_8 = []\n",
        "test_accuracies_8 = []\n",
        "test_minus_8= []\n",
        "norms_8 = []\n",
        "Attn_wk_m_8=[]\n",
        "Attn_wq_m_8=[]\n",
        "Attn_wv_m_8=[]\n",
        "for epoch in tqdm(range(50000)):\n",
        "  train_loss = full_loss(model_8, train, device)\n",
        "  scheduler.step(train_loss)\n",
        "  if epoch % 30 == 0:\n",
        "     for param_tensor in model_8.state_dict():\n",
        "          if param_tensor=='blocks.0.attn.key_matrix.weight':\n",
        "            Attn_wk_m_8.append(model_8.state_dict()[param_tensor].cpu().detach().numpy())\n",
        "          if param_tensor=='blocks.0.attn.query_matrix.weight':\n",
        "            Attn_wq_m_8.append(model_8.state_dict()[param_tensor].cpu().detach().numpy())\n",
        "          if param_tensor=='blocks.0.attn.value_matrix.weight':\n",
        "            Attn_wv_m_8.append(model_8.state_dict()[param_tensor].cpu().detach().numpy())\n",
        "     with torch.no_grad():\n",
        "          log_steps_8.append(epoch)\n",
        "          test_loss = full_loss(model_8, test, device)\n",
        "          test_2,train_2=test_loss.item(),train_loss.item()\n",
        "          train_losses_8.append(train_loss.item())\n",
        "          test_losses_8.append(test_loss.item())\n",
        "          train_accuracies_8.append(full_accuracy(model_8, train, device))\n",
        "          test_accuracies_8.append(full_accuracy(model_8, test, device))\n",
        "          norms_8.append(np.sqrt(sum(param.pow(2).sum().item() for param in model_8.parameters())))\n",
        "          print(\"epoch: %d  | Train loss: %.6f |  Test loss: %.6f | train_acc: %.2f | test_acc: %.2f | l2: %.6f\"%(epoch, train_losses_8[-1], test_losses_8[-1], train_accuracies_8[-1], test_accuracies_8[-1], norms_8[-1]))\n",
        "  train_loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure\n",
        "plt.plot(log_steps_8,train_losses_8, color=Z[1],label='Train losses')\n",
        "plt.plot(log_steps_8,test_losses_8, color=Z[7],label='Test losses')\n",
        "plt.legend(fontsize='large')\n",
        "plt.xlabel('Number Of Epochs')\n",
        "#plt.ylabel('Train/Test Accuracies')\n",
        "plt.ylabel('Train/Test Losses')\n",
        "plt.xscale('log')\n",
        "plt.xlim(1, 5*10**4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D9-JgffVUzBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure\n",
        "plt.plot(log_steps_8,train_accuracies_8, color=Z[1],label='Train accuracies')\n",
        "plt.plot(log_steps_8,test_accuracies_8, color=Z[7],label='Test accuracies')\n",
        "plt.ylabel('Train/Test Accuracies')\n",
        "plt.xlabel('Number Of Epochs')\n",
        "plt.xscale('log')\n",
        "plt.xlim(1, 5*10**4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P8-zzzPRUzBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank_wk_8=[np.linalg.matrix_rank(x) for x in Attn_wk_m_4]\n",
        "rank_wq_m_8=[np.linalg.matrix_rank(x) for x in Attn_wq_m_4]\n",
        "rank_wv_m_8=[np.linalg.matrix_rank(x) for x in Attn_wv_m_4]"
      ],
      "metadata": {
        "id": "iHt4FGPJUzBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rk_wk_4=[x/128 for x in rank_wk_4]\n",
        "rk_wq_4=[x/128 for x in rank_wq_m_4]\n",
        "rk_wv_4=[x/128 for x in rank_wv_m_4]"
      ],
      "metadata": {
        "id": "myLUlpPxUzBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eigenvalues_4, eigenvectors_4 = np.linalg.eig(Attn_wv_m_4[-1])"
      ],
      "metadata": {
        "id": "xZnqNtUtUzBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax=plt.subplots(nrows=1,ncols=2,dpi=100,figsize=[15,8])\n",
        "\n",
        "ax[0].plot(log_steps_4, rk_wk_4, color=Z[0], linewidth=3, label='W_K rank',zorder=4)\n",
        "ax[0].plot(log_steps_4, rk_wv_4, color=Z[3],linewidth=3,label='W_V rank',zorder=1)\n",
        "ax[0].plot(log_steps_4, rk_wq_4, color=Z[1],linewidth=3, label='W_Q rank',zorder=3)\n",
        "ax[0].plot(log_steps_4, train_accuracies_4, color=Z[6],linewidth=3, label='Train accuracies',zorder=3)\n",
        "ax[0].plot(log_steps_4, test_accuracies_4, color=Z[7],linewidth=3, label='Test accuracies',zorder=3)\n",
        "ax[0].legend()\n",
        "ax[0].set_ylabel('Train/Test Accuracies And Rank of attention matrices',fontsize='large')\n",
        "ax[0].set_xlabel(\"Number of epochs\",fontsize='large')\n",
        "ax[0].set_xlim(1, 5*10**4)\n",
        "ax[0].set_xscale('log')\n",
        "ax[0].legend(fontsize='large')\n",
        "#ax[1].set_title(\"Rank dynamics of attention matrices in the second layer\".capitalize(),fontsize='large')\n",
        "\n",
        "ax[1].plot(log_steps_4, train_losses_4, color=Z[5],linewidth=3, label='Train loss',zorder=3)\n",
        "ax[1].plot(log_steps_4, test_losses_4, color=Z[4],linewidth=3, label='Test loss',zorder=3)\n",
        "#ax[2].plot(log_steps, rk_wk, color=Z[0], linewidth=3, label='W_K rank',zorder=4)\n",
        "#ax[2].plot(log_steps, rk_wv, color=Z[3],linewidth=3,label='W_V rank',zorder=1)\n",
        "#ax[2].plot(log_steps, rk_wq, color=Z[1],linewidth=3, label='W_Q rank',zorder=3)\n",
        "ax[1].legend()\n",
        "ax[1].set_ylabel('Train/Test Losses',fontsize='large')\n",
        "ax[1].set_xlabel(\"Number of epochs\",fontsize='large')\n",
        "ax[1].set_xlim(1, 5*10**4)\n",
        "ax[1].set_xscale('log')\n",
        "##ax[2].set_yscale('log')\n",
        "ax[1].legend(fontsize='large')\n",
        "#ax[2].set_title(\"Rank dynamics of attention matrices in the second layer\".capitalize(),fontsize='large')"
      ],
      "metadata": {
        "id": "YniSGKW7UzBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure\n",
        "plt.scatter(np.real(eigenvalues_4), np.imag(eigenvalues_4), marker='o', color=Z[1])\n",
        "plt.axhline(0, color='black',linewidth=0.01)\n",
        "plt.axvline(0, color='black',linewidth=0.01)\n",
        "plt.grid(color = 'gray', linestyle = '--', linewidth = 0.01)\n",
        "plt.xlabel('Eigenvalues of V')\n",
        "plt.legend()\n",
        "plt.legend(fontsize='large')"
      ],
      "metadata": {
        "id": "7C5SxxrHUzBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gPTz4cYOUzBv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "-OOT42OluMcC",
        "bAUdpAMIC1-w",
        "r2HgOcrFGd4E",
        "G90QwHKiGddr",
        "1KxaSLsKtsUK",
        "rPEFz3X4txKk",
        "6VuWcmB_zy5W",
        "xPoaGh2Ut2Kk",
        "ueM5rex99Ngp",
        "XEt4fksG9Ngy",
        "nMsv3OsnoXn4",
        "WUAEQK5moXoA",
        "sDvOMxO1oXoA",
        "yuFwgjWroXoB",
        "_KzBFRamUzBk",
        "GUtdShTSUzBt"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bb9ee555ac7d474e98d3d5ee87ee376c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d896a81851904403808ee9fda8edeffe",
              "IPY_MODEL_b63537448347441a8386706c17ae5555",
              "IPY_MODEL_f4032cf40ecf4182b970fae2dfe588a8"
            ],
            "layout": "IPY_MODEL_84ed2ef4e608452d9d2ad0d9a5381022"
          }
        },
        "d896a81851904403808ee9fda8edeffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b26742c7d774bed93893f544d58f431",
            "placeholder": "​",
            "style": "IPY_MODEL_17e635e867c64dd084dd7e613ff742e8",
            "value": " 12%"
          }
        },
        "b63537448347441a8386706c17ae5555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdf7a358970a4ceca33ae3d4ffed20b7",
            "max": 30000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b099bc6706cf49299397025322dc6211",
            "value": 3608
          }
        },
        "f4032cf40ecf4182b970fae2dfe588a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b2a3cb539b141c3a2d0b2e2e3de12d7",
            "placeholder": "​",
            "style": "IPY_MODEL_84a93899823844228c75077deb8e4a41",
            "value": " 3608/30000 [06:13&lt;44:14,  9.94it/s]"
          }
        },
        "84ed2ef4e608452d9d2ad0d9a5381022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b26742c7d774bed93893f544d58f431": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17e635e867c64dd084dd7e613ff742e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdf7a358970a4ceca33ae3d4ffed20b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b099bc6706cf49299397025322dc6211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b2a3cb539b141c3a2d0b2e2e3de12d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84a93899823844228c75077deb8e4a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}